{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data training and tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_name = 'maguirr4-uo'\n",
    "repo_name = 'cis423'\n",
    "source_file = 'library.py'\n",
    "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
    "!rm $source_file\n",
    "!wget $url\n",
    "%run -i $source_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get future_df to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/maguirr4-uo/cis423/main/EmployeeFuture.csv'\n",
    "future_df = pd.read_csv(url)\n",
    "future_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "future_features = future_df.drop(columns=['LeaveOrNot'])\n",
    "labels = future_df['LeaveOrNot'].to_list()\n",
    "labels[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "future_transformer = Pipeline(steps=[\n",
    "    ('education', MappingTransformer('Education', {'Bachelors': 0, 'Masters': 1, 'PHD': 2})),\n",
    "    ('year', MappingTransformer('JoiningYear', {'2012': 6, '2013': 5, '2014': 4, '2015': 3, '2016': 2, '2017': 1, '2018': 0,})),\n",
    "    ('gender', MappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
    "    ('benched', MappingTransformer('EverBenched', {'No': 0, 'Yes': 1})),\n",
    "    ('ohe', OHETransformer('City')),\n",
    "    ('age', TukeyTransformer('Age', 'outer')),\n",
    "    ('exp', TukeyTransformer('ExperienceInCurrentDomain', 'outer')),\n",
    "    ('scale', MinMaxTransformer()), \n",
    "    ], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(future_features, labels, test_size=0.2, shuffle=True,\n",
    "#                                                   random_state=9, stratify=future_df['LeaveOrNot'])\n",
    "\n",
    "X_train, y_train, X_test, y_test = dataset_setup(future_features, labels, future_transformer, rs=9, ts=.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid = dict(n_neighbors=range(5,100,10),\n",
    "                weights=['uniform', 'distance'],\n",
    "                algorithm=['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                p=[1,2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "grid_result = halving_search(KNeighborsClassifier(), knn_grid, X_train, y_train, factor=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn_model = grid_result.best_estimator_\n",
    "best_knn_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yraw = best_knn_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = threshold_results(np.linspace(0,1,19,endpoint=True), y_test, yraw)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('knn_thresholds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(best_knn_model, 'knn_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_L2 = dict( \n",
    "                penalty=['l2'],                              \n",
    "                solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                max_iter=range(50,1000,50),\n",
    "                class_weight=['balanced', None]\n",
    ")\n",
    "\n",
    "penalty_L1 = dict(\n",
    "                penalty=['l1'],\n",
    "                solver=['liblinear', 'saga'],\n",
    "                max_iter=range(50,1000,50),\n",
    "                class_weight=['balanced', None]\n",
    ")\n",
    "\n",
    "penalty_elasticnet = dict(\n",
    "                penalty=['elasticnet'],\n",
    "                solver=['saga'],\n",
    "                max_iter=range(50,1000,50),\n",
    "                class_weight=['balanced', None],\n",
    "                l1_ratios=[[0.2], [0.5], [0.8]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "logreg_model = LogisticRegressionCV()\n",
    "\n",
    "grids = [penalty_L1, penalty_L2, penalty_elasticnet]\n",
    "\n",
    "grid_result = halving_search(logreg_model, grids, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg_model = grid_result.best_estimator_\n",
    "best_logreg_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yraw = best_logreg_model.predict_proba(X_test)[:,1]\n",
    "result_df = threshold_results(np.linspace(0,1,19,endpoint=True), y_test, yraw)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('logreg_thresholds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(best_logreg_model, 'logreg_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {\n",
    "    \"n_estimators\": range(10,201,10),  #number of trees\n",
    "    \"max_depth\": range(1,15),              #max tree depth\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3, 0.4],\n",
    "    \"subsample\": [.25, .5, 0.75],  # Fix subsample\n",
    "    \"booster\": ['dart', 'gbtree', 'gblinear'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "grid_result = halving_search(xgb_model, xgb_grid, X_train, y_train)\n",
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yraw = best_model.predict_proba(X_test)[:,1]\n",
    "result_df = threshold_results(np.linspace(0,1,19,endpoint=True), y_test, yraw)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('xgb_thresholds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(best_model, 'xgb_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',  #or binary_accuracy\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = tf.keras.metrics.AUC(from_logits=False)  #using roc auc to be consistent with prior models https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC\n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)  #https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy, https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\n",
    "feature_n = len(X_train[0])  # Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_build_binary_model(*, n:int, architecture, metrics=auc, loss=loss, learning_rate=.02):\n",
    "  assert isinstance(n, int), f'n is an int, the number of columns/features of each sample. Instead got {type(n)}'\n",
    "  assert isinstance(architecture, list) or isinstance(architecture, tuple), f'architecture is a list or tuple, the number of nodes per layer. Instead got {type(architecture)}'\n",
    "  assert architecture, f'architecture is empty'\n",
    "  assert isinstance(architecture[0], list), f'architecture should be list of one or more lists but instead {architecture}'\n",
    "\n",
    "  l2_regu = tf.keras.regularizers.L2(0.01)  #weight regularization during gradient descent\n",
    "  initializer = tf.keras.initializers.HeNormal(seed=1234)  #works best with Relu: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  # handle first hidden layer separately because of input_dim\n",
    "\n",
    "  layer_units = architecture[0][0]\n",
    "  layer_dropout = architecture[0][1]\n",
    "  layer_act = architecture[0][2]\n",
    "  model.add(Dense(units=layer_units, activation=layer_act, activity_regularizer=l2_regu, kernel_initializer=initializer, input_dim=n))  #first hidden layer needs number of inputs\n",
    "  model.add(Dropout(layer_dropout))\n",
    "\n",
    "  for layer in architecture[1:]:\n",
    "    layer_units = layer[0]\n",
    "    layer_dropout = layer[1]\n",
    "    layer_act = layer[2]\n",
    "    model.add(Dense(units=layer_units, activation=layer_act, activity_regularizer=l2_regu, kernel_initializer=initializer))\n",
    "    model.add(Dropout(layer_dropout))\n",
    "    \n",
    "  # output layer\n",
    "  model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "  model.compile(loss=loss,\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[metrics])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "model_builder = KerasClassifier(build_fn=ann_build_binary_model, verbose=0)  # Wrap the model to use Sklearn on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = 'elu' # Worked in prior testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_1 = [[16, .2, 'relu']]\n",
    "architecture_2 = [[16, .2, 'relu'], [8, .4, 'relu']]\n",
    "architecture_3 = [[4, .2, 'relu']]\n",
    "architecture_4 = [[16, .4, 'relu'], [8, .2, 'relu']]\n",
    "architecture_5 = [[8, .4, 'relu'], [4, .6, 'relu']]\n",
    "architecture_6 = [[16, .8, 'relu']]\n",
    "architecture_7 = [[16, .2, 'relu'], [8, .4, 'relu']]\n",
    "architecture_8 = [[4, .4, 'relu']]\n",
    "\n",
    "architectures = [architecture_1, architecture_2, architecture_3, architecture_4, architecture_5, architecture_6, architecture_7, architecture_8]\n",
    "\n",
    "# Generating random numbers from range\n",
    "learn_rate = np.random.uniform(low=1e-2, high=1e-4, size=5)  #generate 5 choices in low/high range\n",
    "batch_size = np.random.randint(10,200,5)  #generate 5 choices between 10 and 200\n",
    "epochs = np.random.randint(10,50,5)  #generate 5 choices between 10, 50\n",
    "\n",
    "param_grid = dict(n=[feature_n],\n",
    "                  architecture=architectures,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  learning_rate=learn_rate)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher_model = RandomizedSearchCV(estimator=model_builder, n_jobs=1,  #errors with n_jobs=-1\n",
    "                              cv=5,  #does stratification by default\n",
    "                              verbose=1,\n",
    "                              n_iter=50, #number of random samples to try\n",
    "                              random_state=1234,\n",
    "\t                            param_distributions=param_grid, scoring=\"roc_auc\")\n",
    "\n",
    "searcher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchResults = searcher_model.fit(X_train, y_train, callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchResults.best_score_  #from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestParams = searchResults.best_params_\n",
    "bestParams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = ann_build_binary_model(n=bestParams['n'],\n",
    "                                   architecture=bestParams['architecture'],\n",
    "                                   learning_rate=bestParams['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = ann_model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                         batch_size=bestParams['batch_size'],\n",
    "                         epochs=bestParams['epochs'],\n",
    "                         verbose=0,\n",
    "                         callbacks=[early_stop_cb],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yraw = ann_model.predict(X_test)[:,0]  #replaces predict_proba\n",
    "yraw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = [1 if y>.5 else 0 for y in yraw]  # use normal threshold of .5\n",
    "sum([x==y for x,y in zip(binary,y_test)])/len(binary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the treshold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yraw = ann_model.predict(X_test)\n",
    "result_df = threshold_results(np.linspace(0,1,19,endpoint=True), y_test, yraw)\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I saved to my local Google Drive storage after this. All models are still in there."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
